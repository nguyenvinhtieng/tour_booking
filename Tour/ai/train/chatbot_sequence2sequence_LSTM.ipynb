{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpSgN-7z7H2"
      },
      "source": [
        "# Sequence2Sequence - Mô hình LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:00.287893Z",
          "iopub.status.busy": "2022-05-19T04:06:00.287605Z",
          "iopub.status.idle": "2022-05-19T04:06:02.752634Z",
          "shell.execute_reply": "2022-05-19T04:06:02.751817Z",
          "shell.execute_reply.started": "2022-05-19T04:06:00.287844Z"
        },
        "id": "s8CRgGNdz7H6",
        "outputId": "3f84d00e-1bf6-48c4-a856-a32331344b05",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from underthesea import word_tokenize \n",
        "\n",
        "from keras import Input, Model\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m85Nht96z7H8"
      },
      "source": [
        "# Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:02.755330Z",
          "iopub.status.busy": "2022-05-19T04:06:02.755039Z",
          "iopub.status.idle": "2022-05-19T04:06:02.792211Z",
          "shell.execute_reply": "2022-05-19T04:06:02.791533Z",
          "shell.execute_reply.started": "2022-05-19T04:06:02.755293Z"
        },
        "id": "Ha9Wle5qz7H8",
        "outputId": "01a90729-f6d6-4a79-d430-1a519679db5b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_csv('./data.csv', usecols=[1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:02.794288Z",
          "iopub.status.busy": "2022-05-19T04:06:02.793891Z",
          "iopub.status.idle": "2022-05-19T04:06:02.799132Z",
          "shell.execute_reply": "2022-05-19T04:06:02.798366Z",
          "shell.execute_reply.started": "2022-05-19T04:06:02.794252Z"
        },
        "id": "gyw4fdENz7H9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# lấy ra Questions và Answers\n",
        "data_questions = df['user_a'].values\n",
        "data_answers = df['user_b'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:02.801177Z",
          "iopub.status.busy": "2022-05-19T04:06:02.800342Z",
          "iopub.status.idle": "2022-05-19T04:06:02.808563Z",
          "shell.execute_reply": "2022-05-19T04:06:02.807877Z",
          "shell.execute_reply.started": "2022-05-19T04:06:02.801129Z"
        },
        "id": "IODVl3OPz7H9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# hàm để xóa các ký tự đặc biệt\n",
        "def clean_text(sent):\n",
        "    return re.sub(r'[!“”\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', '', sent)\n",
        "\n",
        "# hàm để chuyển Word Segmentation cho tiếng Việt\n",
        "def clean_and_word_segmentation(sent):\n",
        "    return word_tokenize(clean_text(sent.lower()), format='text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:02.810427Z",
          "iopub.status.busy": "2022-05-19T04:06:02.809966Z",
          "iopub.status.idle": "2022-05-19T04:06:03.268935Z",
          "shell.execute_reply": "2022-05-19T04:06:03.268253Z",
          "shell.execute_reply.started": "2022-05-19T04:06:02.810392Z"
        },
        "id": "LBPjzhpiz7H-",
        "outputId": "afa53dd0-63c5-4629-b06c-e39a463e882c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# vẽ biểu đồ thể hiện trực quan số lượng từ trong các questions\n",
        "count_words_ques = [len(clean_text(ques).split()) for ques in data_questions]\n",
        "counter_words_ques = Counter(count_words_ques)\n",
        "\n",
        "# list_count_word = []\n",
        "# list_count_sent = []\n",
        "# for i in counter_words_ques.items():\n",
        "#     #print(i)\n",
        "#     list_count_word.append(i[0])\n",
        "#     list_count_sent.append(i[1])\n",
        "    \n",
        "# # ========== draw ========== #\n",
        "# plt.figure(figsize=(15,10))\n",
        "# plt.bar(list_count_word,list_count_sent)\n",
        "# plt.title('The graph shows the number of words in sentences')\n",
        "# plt.xlabel('Number of words in a sentence')\n",
        "# plt.ylabel('Number of sentences')\n",
        "# plt.xticks(range(min(list_count_word), max(list_count_word),2))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:03.270805Z",
          "iopub.status.busy": "2022-05-19T04:06:03.270396Z",
          "iopub.status.idle": "2022-05-19T04:06:03.276881Z",
          "shell.execute_reply": "2022-05-19T04:06:03.276169Z",
          "shell.execute_reply.started": "2022-05-19T04:06:03.270766Z"
        },
        "id": "-jmmRMgDz7H-",
        "outputId": "eba88b0e-c213-4b39-c0a2-4f0cb44b7326",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7804, 7804)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(count_words_ques), len(data_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:03.278836Z",
          "iopub.status.busy": "2022-05-19T04:06:03.278128Z",
          "iopub.status.idle": "2022-05-19T04:06:03.294327Z",
          "shell.execute_reply": "2022-05-19T04:06:03.293356Z",
          "shell.execute_reply.started": "2022-05-19T04:06:03.278798Z"
        },
        "id": "3QXRa8swz7H_",
        "outputId": "bda963aa-0e1c-43b2-88fa-fe2cfd79c6a5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# loại bỏ những câu có số lượng từ > 30 từ\n",
        "sorted_ques = []\n",
        "sorted_ans = []\n",
        "for i,count in enumerate(count_words_ques):\n",
        "    if count <= 30:\n",
        "        sorted_ques.append(data_questions[i])\n",
        "        sorted_ans.append(data_answers[i])\n",
        "        \n",
        "# print('len sorted_ques:', len(sorted_ques))\n",
        "# print('len sorted_ans:', len(sorted_ans))\n",
        "# sorted_ques[:2], sorted_ans[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:03.296508Z",
          "iopub.status.busy": "2022-05-19T04:06:03.295946Z",
          "iopub.status.idle": "2022-05-19T04:06:06.533555Z",
          "shell.execute_reply": "2022-05-19T04:06:06.532697Z",
          "shell.execute_reply.started": "2022-05-19T04:06:03.296444Z"
        },
        "id": "-4og7ljDz7H_",
        "outputId": "c2efaf7b-2021-4059-f08f-50ad9c5281f7",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['có bao_nhiêu loại tour khác nhau', 'tour hạng vip có_giá bao_nhiêu'],\n",
              " ['<START> có ba loại tour khác nhau hạng vip hạng tiêu_chuẩn và hạng phổ_thông <END>',\n",
              "  '<START> tour hạng vip có_giá là 900 đơn_vị_tiền_tệ <END>'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# làm sạch và Word Segmentation cho sorted_ques và sorted_ans\n",
        "questions = [clean_and_word_segmentation(ques) for ques in sorted_ques]\n",
        "answers = ['<START> '+ clean_and_word_segmentation(answ) + ' <END>' for answ in sorted_ans]\n",
        "\n",
        "questions[:2], answers[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:06.538150Z",
          "iopub.status.busy": "2022-05-19T04:06:06.537462Z",
          "iopub.status.idle": "2022-05-19T04:06:06.751362Z",
          "shell.execute_reply": "2022-05-19T04:06:06.750504Z",
          "shell.execute_reply.started": "2022-05-19T04:06:06.538107Z"
        },
        "id": "5MdUnxcrz7IA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tokenize cho questions và answers\n",
        "tokenizer = Tokenizer(filters='', lower=False)  # filters='' do dữ liệu đã được làm sạch và giữ lại word segmention\n",
        "tokenizer.fit_on_texts(questions + answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vd7V0qQz7IA"
      },
      "outputs": [],
      "source": [
        "# for word_index in tokenizer.word_index.items():\n",
        "#     print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:06.756453Z",
          "iopub.status.busy": "2022-05-19T04:06:06.755902Z",
          "iopub.status.idle": "2022-05-19T04:06:06.765782Z",
          "shell.execute_reply": "2022-05-19T04:06:06.765112Z",
          "shell.execute_reply.started": "2022-05-19T04:06:06.756413Z"
        },
        "id": "8A6ZJ5RMz7IB",
        "outputId": "04e425ca-9d2c-4291-a003-503bd1b2b388",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size : 5043\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size : {VOCAB_SIZE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PAZlYRfz7IB",
        "outputId": "0947d577-a192-4499-9b1d-e2d68cbb757a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7792,)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:06.773812Z",
          "iopub.status.busy": "2022-05-19T04:06:06.771780Z",
          "iopub.status.idle": "2022-05-19T04:06:06.893705Z",
          "shell.execute_reply": "2022-05-19T04:06:06.893019Z",
          "shell.execute_reply.started": "2022-05-19T04:06:06.773776Z"
        },
        "id": "BA8OHBGpz7IB",
        "outputId": "032e11f7-7104-4763-cf31-2bd92c735fce",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7792, 15)\n",
            "['có bao_nhiêu loại tour khác nhau', 'tour hạng vip có_giá bao_nhiêu', 'giá của tour hạng tiêu_chuẩn là gì', 'tour hạng phổ_thông có_giá là bao_nhiêu', 'tour nào có số_lượng người tham_gia tối_đa']\n",
            "[[4, 26, 211, 16, 165, 157], [16, 40, 121, 141, 26], [65, 38, 16, 40, 126, 10, 7], [16, 40, 168, 141, 10, 26], [16, 13, 4, 209, 29, 218, 281]]\n",
            "[[  4  26 211  16 165 157   0   0   0   0   0   0   0   0   0]\n",
            " [ 16  40 121 141  26   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 65  38  16  40 126  10   7   0   0   0   0   0   0   0   0]\n",
            " [ 16  40 168 141  10  26   0   0   0   0   0   0   0   0   0]\n",
            " [ 16  13   4 209  29 218 281   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "# encoder\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = 15\n",
        "encoder_inp = pad_sequences(tokenized_questions,maxlen=maxlen_questions,padding='post')\n",
        "\n",
        "print(encoder_inp.shape)\n",
        "print(questions[:5])\n",
        "print(tokenized_questions[:5])\n",
        "print(encoder_inp[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:06.899666Z",
          "iopub.status.busy": "2022-05-19T04:06:06.897714Z",
          "iopub.status.idle": "2022-05-19T04:06:07.034356Z",
          "shell.execute_reply": "2022-05-19T04:06:07.033681Z",
          "shell.execute_reply.started": "2022-05-19T04:06:06.899626Z"
        },
        "id": "IpXed1Ygz7IC",
        "outputId": "b0dafdc5-8bc5-4146-dfa2-259c9091ca91",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7792, 50)\n",
            "<START> có ba loại tour khác nhau hạng vip hạng tiêu_chuẩn và hạng phổ_thông <END>\n",
            "[1, 4, 287, 211, 16, 165, 157, 40, 121, 40, 126, 42, 40, 168, 2]\n",
            "[  1   4 287 211  16 165 157  40 121  40 126  42  40 168   2   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# decoder\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = np.max([len(x) for x in tokenized_answers])\n",
        "decoder_inp = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "print(decoder_inp.shape)\n",
        "print(answers[0])\n",
        "print(tokenized_answers[0])\n",
        "print(decoder_inp[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:07.040086Z",
          "iopub.status.busy": "2022-05-19T04:06:07.038194Z",
          "iopub.status.idle": "2022-05-19T04:06:07.801136Z",
          "shell.execute_reply": "2022-05-19T04:06:07.800335Z",
          "shell.execute_reply.started": "2022-05-19T04:06:07.040034Z"
        },
        "id": "4kk2hlMOz7IC",
        "outputId": "87a8909f-9481-4343-e0f2-e692a78000b5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7792, 50, 5043)\n",
            "[4, 287, 211, 16, 165, 157, 40, 121, 40, 126, 42, 40, 168, 2]\n",
            "[  4 287 211  16 165 157  40 121  40 126  42  40 168   2   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "(7792, 50, 5043)\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "    \n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_final_output = to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_final_output.shape)\n",
        "print(tokenized_answers[0])\n",
        "print(padded_answers[0])\n",
        "print(decoder_final_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxmBL8qtz7IC",
        "outputId": "a6d11eb7-6e6e-4006-fb93-11f78b0af60b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5043"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:07.802525Z",
          "iopub.status.busy": "2022-05-19T04:06:07.802264Z",
          "iopub.status.idle": "2022-05-19T04:06:10.551895Z",
          "shell.execute_reply": "2022-05-19T04:06:10.551117Z",
          "shell.execute_reply.started": "2022-05-19T04:06:07.802472Z"
        },
        "id": "zmF3U6Wuz7ID",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "output = dec_dense(dec_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:10.553625Z",
          "iopub.status.busy": "2022-05-19T04:06:10.553353Z",
          "iopub.status.idle": "2022-05-19T04:06:10.579608Z",
          "shell.execute_reply": "2022-05-19T04:06:10.578892Z",
          "shell.execute_reply.started": "2022-05-19T04:06:10.553588Z"
        },
        "id": "9lcJaaM2z7ID",
        "outputId": "0e40a3db-abcf-4e9c-d6dd-44bc6b83f881",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 200)            1008600   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 200)            1008600   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 200),          320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'lstm[0][1]',                \n",
            "                              (None, 200)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 5043)           1013643   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3672443 (14.01 MB)\n",
            "Trainable params: 3672443 (14.01 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:06:10.582011Z",
          "iopub.status.busy": "2022-05-19T04:06:10.581748Z",
          "iopub.status.idle": "2022-05-19T04:29:54.188463Z",
          "shell.execute_reply": "2022-05-19T04:29:54.187700Z",
          "shell.execute_reply.started": "2022-05-19T04:06:10.581976Z"
        },
        "id": "Hjh4vm71z7ID",
        "outputId": "21d70ef2-a5ff-4e1b-e437-ee57539907c6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "78/78 [==============================] - 70s 732ms/step - loss: 6.2967\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 56s 724ms/step - loss: 5.2104\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 55s 711ms/step - loss: 4.9277\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 54s 699ms/step - loss: 4.7860\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 54s 699ms/step - loss: 4.7253\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 54s 696ms/step - loss: 4.6814\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 55s 707ms/step - loss: 4.6341\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 56s 725ms/step - loss: 4.5796\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 60s 773ms/step - loss: 4.5309\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 61s 780ms/step - loss: 4.4805\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 53s 679ms/step - loss: 4.4334\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 53s 674ms/step - loss: 4.3985\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 53s 679ms/step - loss: 4.3602\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 54s 697ms/step - loss: 4.3227\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 53s 679ms/step - loss: 4.2911\n",
            "Epoch 16/100\n",
            "78/78 [==============================] - 52s 672ms/step - loss: 4.2631\n",
            "Epoch 17/100\n",
            "78/78 [==============================] - 52s 667ms/step - loss: 4.2286\n",
            "Epoch 18/100\n",
            "78/78 [==============================] - 52s 669ms/step - loss: 4.1999\n",
            "Epoch 19/100\n",
            "78/78 [==============================] - 52s 663ms/step - loss: 4.1668\n",
            "Epoch 20/100\n",
            "78/78 [==============================] - 53s 674ms/step - loss: 4.1403\n",
            "Epoch 21/100\n",
            "78/78 [==============================] - 52s 670ms/step - loss: 4.1124\n",
            "Epoch 22/100\n",
            "78/78 [==============================] - 55s 701ms/step - loss: 4.0808\n",
            "Epoch 23/100\n",
            "78/78 [==============================] - 53s 678ms/step - loss: 4.0552\n",
            "Epoch 24/100\n",
            "78/78 [==============================] - 54s 694ms/step - loss: 4.0245\n",
            "Epoch 25/100\n",
            "78/78 [==============================] - 53s 676ms/step - loss: 3.9956\n",
            "Epoch 26/100\n",
            "78/78 [==============================] - 52s 661ms/step - loss: 3.9681\n",
            "Epoch 27/100\n",
            "78/78 [==============================] - 52s 662ms/step - loss: 3.9397\n",
            "Epoch 28/100\n",
            "78/78 [==============================] - 53s 673ms/step - loss: 3.9121\n",
            "Epoch 29/100\n",
            "78/78 [==============================] - 51s 658ms/step - loss: 3.8854\n",
            "Epoch 30/100\n",
            "78/78 [==============================] - 52s 664ms/step - loss: 3.8538\n",
            "Epoch 31/100\n",
            "78/78 [==============================] - 54s 694ms/step - loss: 3.8291\n",
            "Epoch 32/100\n",
            "78/78 [==============================] - 54s 688ms/step - loss: 3.8012\n",
            "Epoch 33/100\n",
            "78/78 [==============================] - 52s 671ms/step - loss: 3.7706\n",
            "Epoch 34/100\n",
            "78/78 [==============================] - 52s 664ms/step - loss: 3.7392\n",
            "Epoch 35/100\n",
            "78/78 [==============================] - 52s 667ms/step - loss: 3.7105\n",
            "Epoch 36/100\n",
            "78/78 [==============================] - 52s 663ms/step - loss: 3.6812\n",
            "Epoch 37/100\n",
            "78/78 [==============================] - 54s 694ms/step - loss: 3.6517\n",
            "Epoch 38/100\n",
            "78/78 [==============================] - 59s 755ms/step - loss: 3.6221\n",
            "Epoch 39/100\n",
            "78/78 [==============================] - 67s 856ms/step - loss: 3.5946\n",
            "Epoch 40/100\n",
            "78/78 [==============================] - 79s 1s/step - loss: 3.5618\n",
            "Epoch 41/100\n",
            "78/78 [==============================] - 85s 1s/step - loss: 3.5377\n",
            "Epoch 42/100\n",
            "78/78 [==============================] - 81s 1s/step - loss: 3.5075\n",
            "Epoch 43/100\n",
            "78/78 [==============================] - 82s 1s/step - loss: 3.4815\n",
            "Epoch 44/100\n",
            "78/78 [==============================] - 76s 976ms/step - loss: 3.4516\n",
            "Epoch 45/100\n",
            "78/78 [==============================] - 80s 1s/step - loss: 3.4243\n",
            "Epoch 46/100\n",
            "78/78 [==============================] - 68s 870ms/step - loss: 3.3949\n",
            "Epoch 47/100\n",
            "78/78 [==============================] - 71s 909ms/step - loss: 3.3712\n",
            "Epoch 48/100\n",
            "78/78 [==============================] - 68s 871ms/step - loss: 3.3417\n",
            "Epoch 49/100\n",
            "78/78 [==============================] - 71s 904ms/step - loss: 3.3181\n",
            "Epoch 50/100\n",
            "78/78 [==============================] - 66s 852ms/step - loss: 3.2868\n",
            "Epoch 51/100\n",
            "78/78 [==============================] - 69s 879ms/step - loss: 3.2615\n",
            "Epoch 52/100\n",
            "78/78 [==============================] - 75s 957ms/step - loss: 3.2365\n",
            "Epoch 53/100\n",
            "78/78 [==============================] - 81s 1s/step - loss: 3.2087\n",
            "Epoch 54/100\n",
            "78/78 [==============================] - 80s 1s/step - loss: 3.1824\n",
            "Epoch 55/100\n",
            "78/78 [==============================] - 82s 1s/step - loss: 3.1565\n",
            "Epoch 56/100\n",
            "78/78 [==============================] - 99s 1s/step - loss: 3.1306\n",
            "Epoch 57/100\n",
            "78/78 [==============================] - 73s 932ms/step - loss: 3.1036\n",
            "Epoch 58/100\n",
            "78/78 [==============================] - 68s 869ms/step - loss: 3.0792\n",
            "Epoch 59/100\n",
            "78/78 [==============================] - 68s 869ms/step - loss: 3.0549\n",
            "Epoch 60/100\n",
            "78/78 [==============================] - 66s 849ms/step - loss: 3.0307\n",
            "Epoch 61/100\n",
            "78/78 [==============================] - 67s 860ms/step - loss: 3.0036\n",
            "Epoch 62/100\n",
            "78/78 [==============================] - 66s 841ms/step - loss: 2.9794\n",
            "Epoch 63/100\n",
            "78/78 [==============================] - 65s 838ms/step - loss: 2.9573\n",
            "Epoch 64/100\n",
            "78/78 [==============================] - 67s 857ms/step - loss: 2.9310\n",
            "Epoch 65/100\n",
            "78/78 [==============================] - 67s 864ms/step - loss: 2.9045\n",
            "Epoch 66/100\n",
            "78/78 [==============================] - 67s 864ms/step - loss: 2.8826\n",
            "Epoch 67/100\n",
            "78/78 [==============================] - 73s 932ms/step - loss: 2.8564\n",
            "Epoch 68/100\n",
            "78/78 [==============================] - 75s 956ms/step - loss: 2.8318\n",
            "Epoch 69/100\n",
            "78/78 [==============================] - 75s 967ms/step - loss: 2.8074\n",
            "Epoch 70/100\n",
            "78/78 [==============================] - 77s 986ms/step - loss: 2.7836\n",
            "Epoch 71/100\n",
            "78/78 [==============================] - 76s 972ms/step - loss: 2.7570\n",
            "Epoch 72/100\n",
            "78/78 [==============================] - 56s 715ms/step - loss: 2.7370\n",
            "Epoch 73/100\n",
            "78/78 [==============================] - 55s 708ms/step - loss: 2.7133\n",
            "Epoch 74/100\n",
            "78/78 [==============================] - 54s 686ms/step - loss: 2.6912\n",
            "Epoch 75/100\n",
            "78/78 [==============================] - 53s 678ms/step - loss: 2.6671\n",
            "Epoch 76/100\n",
            "78/78 [==============================] - 53s 686ms/step - loss: 2.6418\n",
            "Epoch 77/100\n",
            "78/78 [==============================] - 53s 679ms/step - loss: 2.6190\n",
            "Epoch 78/100\n",
            "78/78 [==============================] - 53s 679ms/step - loss: 2.5958\n",
            "Epoch 79/100\n",
            "78/78 [==============================] - 59s 758ms/step - loss: 2.5719\n",
            "Epoch 80/100\n",
            "78/78 [==============================] - 66s 842ms/step - loss: 2.5469\n",
            "Epoch 81/100\n",
            "78/78 [==============================] - 64s 816ms/step - loss: 2.5255\n",
            "Epoch 82/100\n",
            "78/78 [==============================] - 64s 827ms/step - loss: 2.5003\n",
            "Epoch 83/100\n",
            "78/78 [==============================] - 58s 743ms/step - loss: 2.4805\n",
            "Epoch 84/100\n",
            "78/78 [==============================] - 53s 685ms/step - loss: 2.4559\n",
            "Epoch 85/100\n",
            "78/78 [==============================] - 55s 703ms/step - loss: 2.4317\n",
            "Epoch 86/100\n",
            "78/78 [==============================] - 53s 684ms/step - loss: 2.4100\n",
            "Epoch 87/100\n",
            "78/78 [==============================] - 52s 672ms/step - loss: 2.3892\n",
            "Epoch 88/100\n",
            "78/78 [==============================] - 54s 688ms/step - loss: 2.3615\n",
            "Epoch 89/100\n",
            "78/78 [==============================] - 53s 680ms/step - loss: 2.3446\n",
            "Epoch 90/100\n",
            "78/78 [==============================] - 53s 677ms/step - loss: 2.3189\n",
            "Epoch 91/100\n",
            "78/78 [==============================] - 61s 785ms/step - loss: 2.2938\n",
            "Epoch 92/100\n",
            "78/78 [==============================] - 59s 755ms/step - loss: 2.2735\n",
            "Epoch 93/100\n",
            "78/78 [==============================] - 57s 735ms/step - loss: 2.2528\n",
            "Epoch 94/100\n",
            "78/78 [==============================] - 60s 765ms/step - loss: 2.2305\n",
            "Epoch 95/100\n",
            "78/78 [==============================] - 59s 751ms/step - loss: 2.2094\n",
            "Epoch 96/100\n",
            "78/78 [==============================] - 61s 779ms/step - loss: 2.1859\n",
            "Epoch 97/100\n",
            "78/78 [==============================] - 58s 749ms/step - loss: 2.1630\n",
            "Epoch 98/100\n",
            "78/78 [==============================] - 58s 748ms/step - loss: 2.1433\n",
            "Epoch 99/100\n",
            "78/78 [==============================] - 58s 748ms/step - loss: 2.1213\n",
            "Epoch 100/100\n",
            "78/78 [==============================] - 53s 678ms/step - loss: 2.0992\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_inp, decoder_inp],\n",
        "           decoder_final_output,\n",
        "           batch_size=100,\n",
        "           epochs=100)\n",
        "model.save('sequence2sequence_lstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43e2qBOv0XuO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('chatbot.sav','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:29:54.189908Z",
          "iopub.status.busy": "2022-05-19T04:29:54.189650Z",
          "iopub.status.idle": "2022-05-19T04:29:54.878025Z",
          "shell.execute_reply": "2022-05-19T04:29:54.877273Z",
          "shell.execute_reply.started": "2022-05-19T04:29:54.189862Z"
        },
        "id": "-NgRysaLz7IE",
        "outputId": "12717220-8380-4f17-df95-a40f7be82d06",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 200)            1008600   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 200),          320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'input_5[0][0]',             \n",
            "                              (None, 200)]                           'input_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 5043)           1013643   ['lstm_1[2][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2343043 (8.94 MB)\n",
            "Trainable params: 2343043 (8.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 200)         1008600   \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1329400 (5.07 MB)\n",
            "Trainable params: 1329400 (5.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                            initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs = [dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "def str_to_tokens(sentence):\n",
        "    #words = sentence.lower().split()\n",
        "    words = clean_and_word_segmentation(sentence).split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word)\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list], maxlen=maxlen_questions,padding='post')\n",
        "\n",
        "enc_model, dec_model = make_inference_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:29:54.879534Z",
          "iopub.status.busy": "2022-05-19T04:29:54.879260Z",
          "iopub.status.idle": "2022-05-19T04:29:54.893439Z",
          "shell.execute_reply": "2022-05-19T04:29:54.892512Z",
          "shell.execute_reply.started": "2022-05-19T04:29:54.879495Z"
        },
        "id": "uSnhNJyNz7IE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def chatbot(input):\n",
        "    print('Bot: Xin chào!')\n",
        "    \n",
        "    while True:\n",
        "        input_question = input\n",
        "    \n",
        "        if input_question == '':\n",
        "            print('Bot answer: bye')\n",
        "            break\n",
        "        states_values = enc_model.predict(str_to_tokens(input_question))\n",
        "        empty_target_seq = np.zeros((1,1))\n",
        "        empty_target_seq[0,0] = tokenizer.word_index['<START>']\n",
        "        stop_condition = False\n",
        "        decoded_translation = ''\n",
        "        while not stop_condition:\n",
        "            dec_outputs, h, c = dec_model.predict([empty_target_seq]+states_values)\n",
        "            sampled_word_index = np.argmax(dec_outputs[0,-1, :])\n",
        "            sampled_word = None\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if sampled_word_index == index:\n",
        "                    if word != '<END>':\n",
        "                        decoded_translation += f'{word} '\n",
        "                    sampled_word = word\n",
        "\n",
        "            if sampled_word == '<END>' or len(decoded_translation.split()) > maxlen_answers:\n",
        "                stop_condition = True\n",
        "            empty_target_seq = np.zeros((1,1))\n",
        "            empty_target_seq[0,0] = sampled_word_index\n",
        "            states_values = [h,c]\n",
        "        print('User: ',input_question)\n",
        "        print('Bot answer:', decoded_translation, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-19T04:29:54.896860Z",
          "iopub.status.busy": "2022-05-19T04:29:54.896632Z",
          "iopub.status.idle": "2022-05-19T04:35:08.052376Z",
          "shell.execute_reply": "2022-05-19T04:35:08.051656Z",
          "shell.execute_reply.started": "2022-05-19T04:29:54.896825Z"
        },
        "id": "YGE0d8G3z7IF",
        "outputId": "df5f0dd8-0911-472c-aca5-37c4c49a1cfa",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Xin chào!\n",
            "User:  tên gì\n",
            "Bot answer: hùng  \n",
            "\n",
            "User:  có học đại học không\n",
            "Bot answer: có  \n",
            "\n",
            "User:  có ở kí túc xá trường không?\n",
            "Bot answer: có  \n",
            "\n",
            "User:  trường ở đâu\n",
            "Bot answer: quận 7  \n",
            "\n",
            "User:  học mấy năm rồi?\n",
            "Bot answer: 2 năm  \n",
            "\n",
            "User:  cao nhiêu?\n",
            "Bot answer: 1 m7  \n",
            "\n",
            "Bot answer: bye\n"
          ]
        }
      ],
      "source": [
        "# chat with bot\n",
        "chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3lop7oVpaRz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "enc_model.save(\"enc_model.h5\")\n",
        "dec_model.save(\"dec_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu8duWG8piR_"
      },
      "outputs": [],
      "source": [
        "# enc_model = keras.models.load_model(\"./enc_model.h5\")\n",
        "# dec_model =  keras.models.load_model(\"./dec_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-SWQf6cpi7C"
      },
      "outputs": [],
      "source": [
        "pickle.dump(tokenizer.word_index, open(\"tokenizer_word_index.sav\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gIduLOhp81k"
      },
      "outputs": [],
      "source": [
        "pickle.dump(maxlen_answers, open(\"maxlen_answers.sav\", \"wb\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "a568c93b4c30704bcfc6354a8f9f0b414cb29a99ddf437e4b5853f43be4fcd25"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
